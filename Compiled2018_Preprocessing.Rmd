##This initial code for Filtering, Sequence variants, and Merging/ remove chimeras, taxonomic assignments is modified from Benjamin Callahan, "A DADA2 workflow for Big Data: Paired-end" https://benjjneb.github.io/dada2/bigdata_paired.html to work with my data. 

```{r}
#Load the necessary libraries for preprocessing and analysis:
library(ggplot2)
library(phyloseq); packageVersion("phyloseq")
library(ShortRead)
library(dada2)
library(ape); packageVersion('ape') #library for creating  tree
library(dplyr)
library(vegan)
library(ampvis2)
library(DESeq2)
library(cowplot)
library(grid)
library(Rmisc)
```

```{r}
#Filtering raw files
pathF <- "/FilePath/raw_data_F/" #set path for forward reads
pathR <- "/FilePath/raw_data_R/" #set path for reverse reads
filtpathF <- file.path(pathF, "filtered") # Filtered forward files go into the pathF/filtered/ subdirectory
filtpathR <- file.path(pathR, "filtered") # ...
fastqFs <- sort(list.files(pathF, pattern="fastq"))
fastqRs <- sort(list.files(pathR, pattern="fastq"))
if(length(fastqFs) != length(fastqRs)) stop("Forward and reverse files do not match.")
#filterAndTrim(fwd=file.path(pathF, fastqFs), filt=file.path(filtpathF, fastqFs),
             # rev=file.path(pathR, fastqRs), filt.rev=file.path(filtpathR, fastqRs))

filterAndTrim(fwd=file.path(pathF, fastqFs), filt=file.path(filtpathF, fastqFs), rev=file.path(pathR, fastqRs), filt.rev=file.path(filtpathR, fastqRs), trimLeft = c(40,20), truncLen=c(239,250), maxEE=2, truncQ=2, maxN=0, compress=TRUE, verbose=TRUE)

```

```{r}
#Analyze quality scores:
#These show your quality scores. Score ideally is above 30 (and is in my case for most samples). 20 is the minimum. 
plotQualityProfile(fnFs[[50]])
plotQualityProfile(fnFs[[100]])
plotQualityProfile(fnFs[[150]])
plotQualityProfile(fnFs[[200]])

plotQualityProfile(fnRs[[50]])
plotQualityProfile(fnRs[[100]])
plotQualityProfile(fnRs[[150]])
plotQualityProfile(fnRs[[200]])

```

```{r}
#Infer sequence variants
library(dada2); packageVersion("dada2")
# File parsing
filtpathF <- "/FilePath/raw_data_F/filtered" 
filtpathR <- "/FilePath/raw_data_R/filtered" 
filtFs <- list.files(filtpathF, pattern="fastq", full.names = TRUE)
filtRs <- list.files(filtpathR, pattern="fastq", full.names = TRUE)
sample.names <- sapply(strsplit(basename(filtFs), "_"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
sample.namesR <- sapply(strsplit(basename(filtRs), "_"), `[`, 1) # Assumes filename = samplename_XXX.fastq.gz
if(!identical(sample.names, sample.namesR)) stop("Forward and reverse files do not match.")
names(filtFs) <- sample.names
names(filtRs) <- sample.names
set.seed(100)
# Learn forward error rates
errF <- learnErrors(filtFs, multithread=TRUE)
# Learn reverse error rates
errR <- learnErrors(filtRs, multithread=TRUE)
# Sample inference and merger of paired-end reads
mergers <- vector("list", length(sample.names))
names(mergers) <- sample.names

```

```{r}
#Sample inference and merger of paired-end reads
#Here we will merge as 16S first
for(sam in sample.names) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=TRUE)
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=TRUE)
    merger <- mergePairs(ddF, derepF, ddR, derepR,maxMismatch = 0, minOverlap = 5, justConcatenate = FALSE, verbose=TRUE, returnRejects = FALSE)
    mergers[[sam]] <- merger
}
```

```{r}
#Sample inference and merger of paired-end reads
#Here we will merge as 18S 
mergers18 <- vector("list", length(sample.names))
names(mergers18) <- sample.names
for(sam in sample.names) {
  cat("Processing:", sam, "\n")
    derepF <- derepFastq(filtFs[[sam]])
    ddF <- dada(derepF, err=errF, multithread=TRUE)
    derepR <- derepFastq(filtRs[[sam]])
    ddR <- dada(derepR, err=errR, multithread=TRUE)
    EukMerger <- mergePairs(ddF, derepF, ddR, derepR,maxMismatch = 0, minOverlap = 5, justConcatenate = TRUE, verbose=TRUE, returnRejects = FALSE)
    mergers18[[sam]] <- EukMerger
}
```

```{r}
#16S sequence table with chimeras removed
seqtab <- makeSequenceTable(mergers)

#18S sequence table with chimeras removed
seqtab18 <- makeSequenceTable(mergers18)

#Save files
#saveRDS(seqtab, "/FilePath/seqtab.rds") 
#saveRDS(seqtab18, "/FilePath/seqtab18.rds") 
```


```{r}
#Remove chimeras, assign taxonomy
library(dada2); packageVersion("dada2")
# Remove chimeras for 16S
seqtab <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE)
#Remove chimeras for 18S
seqtab18 <- removeBimeraDenovo(seqtab18, method="consensus", multithread=TRUE)
# Assign taxonomy for 16S
tax <- assignTaxonomy(seqtab, "/FilePath/silva_nr_v128_train_set.fa", multithread=TRUE)
#Assign taxonomy for 18S
tax18 <- assignTaxonomy(seqtab18, "/FilePath/silva_nr_v128_train_set.fa", multithread=TRUE)
# Write to disk
#saveRDS(seqtab, "/FilePath/seqtab_final.rds") 
#saveRDS(tax, "FilePath/tax_final.rds")
```

```{r}
##write taxonomy assignments to file...16S
write.csv(tax, file = "taxa_silva_16S.csv")
##write taxonomy assignments to file...18S
write.csv(tax18, file = "taxa_silva_18S.csv")

##save unique sequences to a file for use in alignment. 18S
a<-colnames(otu_table(seqtab18, taxa_are_rows=FALSE))
uniquesToFasta(seqtab18, "18S_uniques.fasta", ids = a)

##save unique sequences to a file for use in alignment.16S
a<-colnames(otu_table(seqtab, taxa_are_rows=FALSE))
uniquesToFasta(seqtab, "16S_uniques.fasta", ids = a)
```
#RUN QIIME CODE IN BETWEEN HERE TO GENERATE TRE FILE
```{r}
#Load into phyloseq FOR 18S
meta = ("/FilePath/Compiled2018MappingFile.txt")
meta = import_qiime_sample_data(meta)
ps_18S <- phyloseq(otu_table(seqtab18, taxa_are_rows=FALSE), 
                  sample_data(meta), 
                  tax_table(tax18))

#read the tree into R and add to the phyloseq object ps.  It's now called ps_t_silva (for phyloseq_tree_silva).**
tree_Q2 = read.tree("/FilePath/18S_uniques_aligned_pfiltered.tre")
tree_Q2 = root(tree_Q2, 1, resolve.root = T)
ps_t_18S = merge_phyloseq(ps_18S,tree_Q2) 

unrare_18S = ps_t_18S

#Run, check all numbers are different.
sample_counts18S = sample_sums(unrare_18S)
```

```{r}
#Load into phyloseq FOR 16S
meta = ("/FilePath/Compiled2018MappingFile.txt")
meta = import_qiime_sample_data(meta)
ps_16S <- phyloseq(otu_table(seqtab, taxa_are_rows=FALSE), 
                  sample_data(meta), 
                  tax_table(tax))

#read the tree into R and add to the phyloseq object ps.  It's now called ps_t_silva (for phyloseq_tree_silva).**
tree_Q2 = read.tree("/FilePath/16S_uniques_aligned_pfiltered.tre")
tree_Q2 = root(tree_Q2, 1, resolve.root = T)
ps_t_16S = merge_phyloseq(ps_16S,tree_Q2) 

unrare_16S = ps_t_16S

#Run, check all numbers are different.
sample_counts16 = sample_sums(unrare_16S)
write.csv(sample_counts16, "sums16.csv")
```

```{r}
#Rarefy to 5638. Be sure to make a CSV and sort from smallest to largest, and just see if there is a clear drop where you can cut off

#Filter out 18S, 16S, Mitochondria and Chloroplasts.
CB16S = subset_taxa(unrare_16S, (Kingdom != "Eukaryota")&(Class != "Chloroplast")&(Family != "Mitochondria"))
sums<-sample_sums(CB16S)
write.csv(sums, "16Ssums.csv")

set.seed(58373)
CB_16S = rarefy_even_depth(CB16S, sample.size = 5638, replace = FALSE, trimOTUs = TRUE)


sample_sums(CB_16S)
write.csv(otu_table(CB16S), "16Sotu.csv")
```

```{r}
#set.seed(58373)
#CB18S = rarefy_even_depth(unrare_18S, sample.size = 6440, replace = FALSE, trimOTUs = TRUE)
#sample_sums(CB18S)
CB_18S = subset_taxa(unrare_18S, Kingdom == "Eukaryota")
sums <-sample_sums(CB_18S)
write.csv(sums, "sums.csv")
set.seed(58373)
#Rarify to 200 
CB_18S = rarefy_even_depth(CB_18S, sample.size = 200, replace = FALSE, trimOTUs = TRUE)
sample_sums(CB_18S)
write.csv(otu_table(CB16S), "18Sotu.csv")
```

```{r}
#FILTER OTU TABLES
#16S, filter samples for analysis

#To make comparisons between needle types for each date.
CB16LS_4= subset_samples(CB_16S, Date=="d_Four")
CB16LS_4_Needles= subset_samples(CB16LS_4, Type=="Needle")
CB16LS_4_Spruce= subset_samples(CB16LS_4_Needles, TreeSpecies=="Spruce")
CB16LS_4_Lodge= subset_samples(CB16LS_4_Needles, TreeSpecies=="Lodge")
CB16LS_4_Control= subset_samples(CB16LS_4, TreeSpecies=="Control")
CB16LS_3= subset_samples(CB_16S, Date=="c_Three")
CB16LS_5= subset_samples(CB_16S, Date=="e_Five")
CB16LS_2= subset_samples(CB_16S, Date=="b_Two")
CB16LS_1= subset_samples(CB_16S, Date=="a_One")
CB16LS_1_Needles= subset_samples(CB16LS_1, Type=="Needle")
CB16LS_1_Spruce= subset_samples(CB16LS_1_Needles, TreeSpecies=="Spruce")

#To make comparisons between dates for each needle type.
CB16LS_Red= subset_samples(CB16_LS_no2red_bot_dubs, Needle=="Red")
CB16LS_Green= subset_samples(CB16_LS_no2red_bot_dubs, Needle=="Green")
CB16LS_Lodge= subset_samples(CB16_LS_no2red_bot_dubs, Needle=="Lodge")
CB16LS_Control= subset_samples(CB16_LS_no2red_bot_dubs, (Needle !="Red")&(Needle !="Green")&(Needle !="Lodge"))

```

```{r}
#18S, filter by sample types

CB18LS_4= subset_samples(CB_18S, Date=="d_Four")
CB18LS_3= subset_samples(CB_18S, Date=="c_Three")
CB18LS_5= subset_samples(CB_18S, Date=="e_Five")
CB18LS_2= subset_samples(CB_18Ss, Date=="b_Two")
CB18LS_1= subset_samples(CB_18S, Date=="a_One")

CB18LS_Red= subset_samples(CB_18S, Needle=="Red")
CB18LS_Green= subset_samples(CB_18S, Needle=="Green")
CB18LS_Lodge= subset_samples(CB_18S, Needle=="Lodge")
CB18LS_Control= subset_samples(CB_18S, Needle=="Control")

```

